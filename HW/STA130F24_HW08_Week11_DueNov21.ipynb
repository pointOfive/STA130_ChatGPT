{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a29dab0",
   "metadata": {},
   "source": [
    "# STA130 Homework 08\n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/STA130_ChatGPT/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eee301",
   "metadata": {},
   "source": [
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "1. Code and write all your answers (for both the \"Prelecture\" and \"Postlecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "    > It is *suggested but not mandatory* that you complete the \"Prelecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "    > Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "\n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>\n",
    "\n",
    "### Marking Rubric (which may award partial credit) ——To be changed \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.3 points]: Assignment completion confirmed by working \"final\" code and ChatBot summaries for \"4\", \"7\", \"9\"\n",
    "- [0.2 points]: Written submission evaluation and enagement confirmation with ChatBot summaries for \"3\", \"6\" \n",
    "- [0.2 points]: Evaluation of engagement and evaluation of written communication in \"8\", \"11\", \"14\"\n",
    "- [0.2 points]: Marked by correctness in \"5\", \"10\", \"13\"\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a53734",
   "metadata": {},
   "source": [
    "### \"Prelecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n",
    "\n",
    "1. Start a ChatBot session to understand what a decision tree is.  Ask ChatBot how decision trees are used in data analysis, the type of problems they can solve, how they make decisions at each node, and provide an example of a real-world application where decision trees might be particularly useful.<br><br>\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>\n",
    "When using chatbots, it's often more effective to ask concise, single questions rather than presenting complex, multi-part queries. This approach can help in obtaining clearer and more specific responses. Additionally, chatbots may not automatically reiterate previously explained concepts. If you need a refresher or further explanation on a topic discussed earlier, it's beneficial to explicitly request this in your prompt or in follow-up interactions.<br><br>Ask for summaries of this ChatBot session and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)<br><br></em>\n",
    "    </details><br>\n",
    "\n",
    "2. Start a new ChatBot session, for each of the following metrics below, provide a scenario where focusing on that particular metric is crucial. Understand why that metric is important in your given scenario and how it influences the decision-making process.<br><br>\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>Ask for summaries of this ChatBot session and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)<br><br></em>\n",
    "    </details><br>  \n",
    "    \n",
    "##### Sensitivity (True Positive Rate)\n",
    "Sensitivity measures the proportion of actual positives that are correctly identified.\n",
    "$$\\text{Sensitivity} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "##### Specificity (True Negative Rate)\n",
    "Specificity measures the proportion of actual negatives that are correctly identified.\n",
    "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
    "\n",
    "##### Accuracy\n",
    "Accuracy measures the proportion of true results (both true positives and true negatives) in the population.\n",
    "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "##### Precision (Positive Predictive Value)\n",
    "Precision measures the proportion of positive identifications that were actually correct.\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c9345",
   "metadata": {},
   "source": [
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "Feel free to work on the \"Postlecture\" HW below if you're making good progress and want to continue: for **HW 08** continuing could be reasonable because questions \"3-6\" below exist as questions to help you review knowledge from before. \n",
    "\n",
    "*The benefits of continue would are that (a) Consolidate the knowledge already learned and integrate it comprehensively. (b) Let you build experience interacting with ChatBots (and understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4893ab",
   "metadata": {},
   "source": [
    "### \"Postlecture\" HW [*submission along with \"Prelecture\" HW is due prior to next TUT*]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58bd64b",
   "metadata": {},
   "source": [
    "3. We begin by importing dataset and the libraries we will use. With ChatBot's help, for each import write one sentence briefly explaining the purpose of the import. Then play around with this data and provide some work demonstrating what the data is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b50624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree, model_selection\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import graphviz as gv\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "ab = pd.read_csv(\"amazonbooks.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0444816",
   "metadata": {},
   "source": [
    "4. Continue with the Chatbot conversation from question 3 and create a sequence of new dataframes `ab_reduced`, `ab_reduced_noNaN`, `ab_reduced_noNaN_train`, and `ab_reduced_noNaN_test` according to the following specifications.\n",
    "\n",
    "\n",
    "> 1. `Weight_oz`, `Width`, and `Height` should be removed because they have a lot of missing values that would cause us to lose rows if they were kept in the analysis; but, we're going to be okay not using them for the analysis. \n",
    "> 2. All rows with `NaN` entries should be dropped.\n",
    "> 3. `Pub year` and `NumPages` should be redefined to have the type `int`, and `Hard_or_Paper` should be redefined to have the type `category`.\n",
    "> 4. Create an 80/20 split with 80% training set and 20% testing set.\n",
    "\n",
    "\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>To complete the final 80/20 split in a reproducible way, set a \"random seed\".<br><br>Only remove rows with `NaN` entries once you've subset to the columns you're interested in. This will minimize potentially unnecessary data loss... Of course we might want to consider imputing missing data to further mitigate data loss, but the considerations for doing so are more advanced than the level of our course, so we'll not consider that for now. At any rate, `NaN` entries can't be used in their raw form with the `scikit-learn` methodologies, so we do need to remove them to proceed with our analyses.<br><br></em>\n",
    "    </details><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225c340",
   "metadata": {},
   "source": [
    "5. How many observations are in the training data set and the test data set?\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>A single observation consists of all the measurements made on a single entity. In Machine Learning, the  \"vector\" of all values measured for a single entity comprise a single \"observation\" so this just corresponds (typically) to a row of a data frame.<br><br></em>\n",
    "    </details><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9906b2d",
   "metadata": {},
   "source": [
    "6. Tell ChatBot that you are about to fit a \"scikit-learn\" `DecisionTreeClassifier` model and ask what the following preparation steps are doing. One you understand the answer, write a one to two sentence summary explaining these preparation steps in your own words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed1122",
   "metadata": {},
   "source": [
    "7. With ChatBot's help:\n",
    "- Train a classification tree `clf` using only the  `List Price` variable to predict whether or not a book is hard cover book or a paper cover book (with  `max_depth` parameter set to `2`).<br><br>\n",
    "- Visualize your decision tree using the `tree.plot_tree(clf)` function shown in the `sklearn` documentation [here](\n",
    "https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#what-is-the-values-array-used-here) and [here](https://scikit-learn.org/stable/modules/tree.html)\n",
    "- Visualize your decision tree again, but this time make it more immediately readible using `graphviz`, which is demonstrated in the `sklearn` documentation [here](https://scikit-learn.org/stable/modules/tree.html#alternative-ways-to-export-trees) <br><br>\n",
    "**Use default values for all (tuning) parameters instantiating the Decision Tree Classifier.**\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>Asking ChatBot about \"DecisionTreeClassifier .fit(...)\" can be helpful here...<br><br>Should you use the \"ab_reduced_noNaN\" data, or the \"ab_reduced_noNaN_train\" data, or the \"ab_reduced_noNaN_test\" data to initially fit the classification tree? Why?<br><br></em>\n",
    "    </details><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3ce1f",
   "metadata": {},
   "source": [
    "8. Assuming you correctly fit the classification tree with the `ab_reduced_noNaN_train` data as opposed to the `ab_reduced_noNaN` or the `ab_reduced_noNaN_test` data sets, why did you do this? Write a one to two sentence answer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b5097",
   "metadata": {},
   "source": [
    "9. This time try on your own, use the same train/test split data used so far: \n",
    "- train classification tree `clf2` with `NumPages`, `Thick` and `List Price` to predict again whether or not a book is hard cover book or a paper cover book (with  `max_depth` parameter set to `4`).\n",
    "- use the same instruction from question 7 to visualize your new tree. <br><br>\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Extra fun stuff here!(You will not be tested for this)</u></summary>\n",
    "<br><em>If you are interested in how to find the best `max_depth` for a tree, ask ChatBot about \"GridSearchCV\".<br><br></em>\n",
    "    </details><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d889386a",
   "metadata": {},
   "source": [
    "10. Use `ab_reduced_noNaN_test` you created in question 4 to create confusion matrices for `clf` and `clf2`. Report the sensitivity (true positive rate), specificity (true negative rate) and accuracy for each of the trees/models.\n",
    "**Provide your answers as decimal numbers with three signifiant digits, such as `0.123` (and not as percentages like `12.3%`).** \n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>\n",
    "    Here are few things that might be helpful to ask ChatBot:<br><br>&emsp;&emsp;&emsp;- How can I use `np.round()`<br><br>&emsp;&emsp;&emsp;- Does the `y_true` or `y_pred` parameter go first in the `confusion_matrix` function?<br><br>&emsp;&emsp;&emsp;- How to label a confusion matrix in `sklearn`<br><br>&emsp;&emsp;&emsp;- Confusion Matrices and Metrics explainations just below, you can always ask ChatBot to explain it in detail or give you some examples<br><br></em>\n",
    "    </details><br>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12218df9",
   "metadata": {},
   "source": [
    "<a id='cf'></a>\n",
    "# Confusion Matrices and Metrics\n",
    "\n",
    "- **Accuracy** is the proportion of cases that are correctly identified.\n",
    "- **Sensitivity** is the proportion of actual positive cases which are correctly identified to be positive (as true positives)\n",
    "    - **Sensitivity** is also known as **true positive rate (TPR)**\n",
    "- **Specificity** is the proportion of actual negative cases which are correctly identified to be negative (as true negative)\n",
    "    - **Specificity** is also known as **true negative rate (TNR)**\n",
    "- **False positive rates (FPR)** are defined to be the proportion of actually negative cases which are incorrectly identified (as false positives)\n",
    "- **False negative rates (FNR)** are defined to be the proportion of actually positive cases which are incorrectly identified (as false negatives)\n",
    "    - *but noticed how the FPR and FNR work in a sort of \"flipped\" manner in these definitions as they are defined with respect to the truth*\n",
    "\n",
    "In formulas\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Accuracy}  & = {} (TP+TN)/\\text{\"total # of cases\"}\\\\\n",
    "TPR & = {} TP/(TP+FN) = 1-FNR \\\\\n",
    "TNR & = {} TN/(TN+FP) = 1-FPR\n",
    "\\end{align*}\n",
    "\n",
    "and you can read more and see a (greatly expanded) handy list of formulas at the following [wikipedia page.](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72576c25",
   "metadata": {},
   "source": [
    "11. Explain what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for `clf` and `clf2`) are better. Write a three to four sentences answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb30e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(ab_reduced_noNaN_train.life_exp_good, \n",
    "                                        clf.predict(ab_reduced_noNaN_train[['List Price']]), labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(confusion_matrix(ab_reduced_noNaN_test.life_exp_good, \n",
    "                                        clf.predict(ab_reduced_noNaN_testtest[['List Price']]), labels=[0, 1]), \n",
    "                       display_labels=[\"Paper\",\"Hard\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526939c2",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Compared to understanding the contribution of different covariates towards the final predicted values of multiple linear regression models (where you can just read off the equation to see how predictions work), the extent to which we do not understand the overall contributions of the different features to the final predictions from our decision trees should feel a bit off-putting. To remedy this we can use so-called **Feature Importance** heuristics to judge how relatively important the different features are in the final decision tree predictions. \n",
    "\n",
    "\n",
    "12. Read the following paragraph and ask ChatBot how to check feature Importance in `scikit-learn`, \n",
    "\n",
    "> The way a decision tree is fit is that at each step in the construction process of adding a new decision node splitting rule to the current tree structure, all possible decision rules for all possible variables are considered, and the one that improves the prediction the most (as measured by the criterion of either \"Gini impurity\" or \"Shannon entropy\") legally and sufficiently according to the tuning parameters rules of the decision tree is added to the decision tree.  The overall \"criterion\" noted above improves with each new decision node splitting rule, so the improvement can thus be tracked and the contributions attributed to the feature upon which the decision node splitting rule is based.  This means the relative contribution of each feature to the overall explanatory power of the model can be calculated, and this is what the `.feature_importances_` attribute does. \n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "<br><em>Ask for summaries of this ChatBot session and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)\n",
    "<br><br></em>\n",
    "    </details><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210dd793",
   "metadata": {},
   "source": [
    "13. Which predictor variable is most important for making predictions according to `clf2`? Report the name of most important feature and its numeric *Feature Importance* value.\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Hint</u></summary>\n",
    "<br><em>&emsp;&emsp;&emsp;- \".feature_importances_\" and \".feature_names_in_\"<br><br>&emsp;&emsp;&emsp;- Visualize Feature Importances : )\n",
    "<br><br></em>\n",
    "    </details><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb7d92c",
   "metadata": {},
   "source": [
    "14. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees. Write a couple sentences to answer this question \n",
    "\n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Hint</u></summary>\n",
    "<br><em>linear model regression predicts continuous real-valued averages for a given configuration of covariate values (or, feature values, if we're using machine learning terminology instead of statistical terminology), whereas a binary classification model such as a binary classification tree predicts 0/1 (\"yes\" or \"no\") outcomes (and gives the probability of a 1 \"yes\" (or \"success\") outcome from which a 1/0 \"yes\"/\"no\" prediction can be made; but, this is not what is being asked here. This question is asking \"what's the difference in the way we can interpret and understand how the predictor variables influence the predictions in linear model regression based on the coefficients versus in binary decision trees based on the Feature Importances?\"\n",
    "<br><br></em>\n",
    "    </details><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2436778",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course \n",
    "\n",
    "<br>\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "1. Discuss with a ChatBox about consent and data collection for training models.\n",
    "    1. Discuss the ethics of data collection for training decision trees, particularly the need for informed consent when personal data is involved.\n",
    "    2. Evaluate the role of regulatory frameworks in ensuring ethical data collection practices.\n",
    "2. Discuss with a ChatBox about accountability in automated decision-making.\n",
    "    1. Address the challenges of holding systems and their developers accountable when decision trees lead to adverse outcomes.\n",
    "    2. Explore legal and ethical frameworks for responsibility when automated decisions go wrong.\n",
    "3. Discuss with a ChatBox about transparency and explainability in classification models.\n",
    "    1. Discuss the importance of model transparency, particularly when using decision trees in sectors like healthcare or criminal justice.\n",
    "    2. Explore methods to enhance the explainability of decision trees, such as visualization techniques and simplified decision paths.\n",
    "4. Discuss with a ChatBox about impact of misclassifications in critical applications.\n",
    "    1. Examine the consequences of false positives and false negatives in decision tree outcomes, using confusion matrices to highlight these issues.\n",
    "    2. Discuss ethical responsibilities when deploying classifiers in high-stakes fields like medicine or law enforcement.\n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\">\n",
    "    <summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "#### Remember to abide by the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) at all times\n",
    "\n",
    "At this point in the course you should be able to create a `clf` loop to give **simple predictions** of one column in the course project data\n",
    "\n",
    "1. At this time, you should have some interesting columns to be used for training a `clf`. Play around and creat as many `clf` as you like!\n",
    "\n",
    "2. Discuss with your team member which `clf` is most appropriate and explain why.\n",
    "   \n",
    "3. Review all the statistical techniques covered in STA130, and consider integrating them holistically into your project!\n",
    "</details>        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524759d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
